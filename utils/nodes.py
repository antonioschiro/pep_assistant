from dotenv import load_dotenv
load_dotenv()
import os
from langchain_core.output_parsers.string import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from pydantic import BaseModel, Field
from langchain_core.documents import Document
from typing import List, Annotated
from typing_extensions import TypedDict
from enum import Enum
from utils.templates import retrieved_document_template, generate_response_template, hallucinations_template, completeness_template, retry_generate_template
# Set the key TOKENIZERS_PARALLELISM to avoid deadlocks
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# Antropic APIKey
os.environ["GROQ_API_KEY"] = os.getenv("CHATGROQ_APIKEY")
# Initialize LLM
llm = ChatGroq(model="meta-llama/llama-4-scout-17b-16e-instruct", temperature=0.0, max_retries=2)
# Defining embeddings generator
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
# Loading vector stores
guidelines_vectorstore = Chroma(persist_directory="./rag_folder/pep_folder", embedding_function=embeddings)
snippets_code_vectorstore = Chroma(persist_directory="./rag_folder/codes_folder", embedding_function=embeddings)
# Setting a global variable to limit the number of iterations:
max_iterations = 3

# Defining evaluators format outputs for Self-RAG
class RetrievedDocumentEvaluator(BaseModel):
    """Evaluates if a retrieved document is relevant for improving code."""
    score: bool = Field(description= "Relevance of document to the code. True or False.")

class HallucinationEvaluator(BaseModel):
    """Evaluates if response generated is affected by hallucinations or none."""
    score: bool = Field(description= '''Hallucination score. 
                        True if LLM is hallucinating, False if the answer is grounded.''')

class CompletenessEvaluator(BaseModel):
    score: bool = Field(description = '''Completeness score.
                        True if the response fully answers to the question. False otherwise.''')

# Defining state graph
# Instanciate an Enum class for answer_state key.
class AnswerState(Enum):
    HALLUCINATED= "hallucinated"
    NOT_COMPLETE= "not complete"
    COMPLETE= "complete"
    NOT_GENERATED= "not generated"

class State(TypedDict):
    code_question: Annotated[str, "The code to be reviewed."]
    retrieved_documents: Annotated[List[Document], "Retrieved documents from vector stores."]
    generated_response: Annotated[str, "Fixed code generated by LLM."]
    iterations: Annotated[int, "Number of iterations for a given user input code."]
    answer_state: Annotated[AnswerState, "The status of the response."]

# Define Langgraph nodes and edges
#Nodes
def retrieve(state: State):
    '''This function takes in input a StateGraph object. 
    It retrieves relevant documents starting from value of state['code_question'] and store them into state['retrieved_documents'] key.'''
    code_question = state['code_question'] # Input code
    answer_state = state['answer_state']
    iterations = state['iterations']
    # Defining how many documents will be retrieved
    pep_docs_number = 5
    codes_doc_number = 3
    # Retrying docs search if the generated answer is not complete.
    if answer_state == AnswerState.NOT_COMPLETE:
        lambda_mult = 0.8        
        if iterations == 2:
            pep_docs_number = 8
            codes_doc_number = 4
        elif iterations > 2:
            lambda_mult = 0.5
            pep_docs_number = 8
            codes_doc_number = 4  
        guidelines_documents=guidelines_vectorstore.max_marginal_relevance_search(query=code_question, k=pep_docs_number, lambda_mult=lambda_mult)
        snippets_documents=snippets_code_vectorstore.max_marginal_relevance_search(query=code_question, k=codes_doc_number, lambda_mult=lambda_mult)
        retrieved_documents=[doc for doc in guidelines_documents+snippets_documents]
        return {"retrieved_documents": retrieved_documents}
    # Retrieving docs: first try.
    guidelines_documents = guidelines_vectorstore.similarity_search(code_question, k=pep_docs_number)
    snippets_documents = snippets_code_vectorstore.similarity_search(code_question, k=codes_doc_number)
    retrieved_documents=[doc for doc in guidelines_documents+snippets_documents]
    return {"retrieved_documents": retrieved_documents}

def evaluate(state: State):
    '''This function checks whether the documents retrieved are useful or not to fix the code.
    It returns the updated key state["retrieved_documents"] with relevant docs only.'''
    code_question = state['code_question']
    retrieved_documents = state['retrieved_documents']
    relevant_docs= []
    # Create prompt from template and feed LLM with it
    evaluation_prompt = PromptTemplate.from_template(retrieved_document_template)
    # Document evaluator chain
    documents_evaluator_chain = (evaluation_prompt
    | llm.with_structured_output(RetrievedDocumentEvaluator))
    for doc in retrieved_documents:
        is_relevant = documents_evaluator_chain.invoke({"code": code_question, "document": doc.page_content})
        if is_relevant.score:
            relevant_docs.append(doc)
    return {"retrieved_documents": relevant_docs}

def generate(state: State):
    '''This function takes in input the state object.
    It updates the state keys "generated_response" and "iterations" with the generated code and the iteration number respectively.
    '''
    iterations = state['iterations'] + 1
    code_question = state['code_question']
    retrieved_docs = state['retrieved_documents']
    answer_state = state['answer_state']
    # Defining PEP rules and code snippets from retrieved docs for passing them to the prompt.
    # Managing the cases with no rules or no code snippets retrieved.
    rules = "No rules found for this input code. Improve the input code using only code snippets."
    code_snippets = "No significant code snippets found for this input code. Improve the input code using only PEP rules and examples."
    # Separating docs according to their metadatas
    pep_docs = [doc for doc in retrieved_docs if doc.metadata['source'].startswith("pep")]
    snippets_docs = [doc for doc in retrieved_docs if not doc.metadata['source'].startswith("pep")]
    if pep_docs:
        rules = '\n'.join(doc.page_content for doc in pep_docs)
    if snippets_docs:
        code_snippets = '\n'.join(doc.page_content for doc in snippets_docs)
    # Retry generation flow in case of hallucinated answer 
    if answer_state == AnswerState.HALLUCINATED:
        hallucinated_response = state['generated_response']
        prompt = PromptTemplate.from_template(generate_response_template + retry_generate_template)
        rag_pipeline = (prompt
                    | llm
                    | StrOutputParser())
        response = rag_pipeline.invoke({'code':code_question, 'rules':rules, 'code_snippets':code_snippets, 'hallucinated_response': hallucinated_response})
        return {"generated_response": response, "iterations": iterations}    
    # Main flow
    # Defining prompt for the main flow
    prompt = PromptTemplate.from_template(generate_response_template)
    rag_pipeline = (prompt
                    | llm
                    | StrOutputParser())
    response = rag_pipeline.invoke({'code': code_question, 'rules':rules, 'code_snippets':code_snippets})
    return {"generated_response": response, "iterations": iterations}

def check_response(state: State):
    code_question=state['code_question']
    retrieved_documents= state['retrieved_documents']
    response= state["generated_response"]
    context = '\n'.join(doc.page_content for doc in retrieved_documents)
    # Hallucination chain
    hallucinations_evaluator_chain = (PromptTemplate.from_template(hallucinations_template)
    | llm.with_structured_output(HallucinationEvaluator))
    # Completeness chain
    completeness_evaluator_chain = (PromptTemplate.from_template(completeness_template)
    | llm.with_structured_output(CompletenessEvaluator))
    # Check for hallucinations
    is_response_hallucinated= hallucinations_evaluator_chain.invoke({"context": context, "response": response})
    if is_response_hallucinated.score:
        return {"answer_state": AnswerState.HALLUCINATED}
    else:
        # Check for completeness
        is_response_complete = completeness_evaluator_chain.invoke({"code": code_question, "response": response})
        if is_response_complete.score:
            return {"answer_state": AnswerState.COMPLETE}
        return {"answer_state": AnswerState.NOT_COMPLETE}

# Edges
def check_relevance(state: State):
    retrieved_documents = state['retrieved_documents']
    if len(retrieved_documents) > 0:
        return "generate"
    return "END"

def route_answer(state: State):
    iterations= state['iterations']
    answer_state = state['answer_state']

    if iterations <= max_iterations:
        # Check for hallucinations
        if answer_state == AnswerState.HALLUCINATED:
            return "generate"
        else:
            # Check for completeness
            if answer_state == AnswerState.COMPLETE:
                return "END"
            return "retrieve"
    print("Max tries reached.")
    return "END"